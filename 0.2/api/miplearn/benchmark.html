<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.5" />
<title>miplearn.benchmark API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>miplearn.benchmark</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#  MIPLearn: Extensible Framework for Learning-Enhanced Mixed-Integer Optimization
#  Copyright (C) 2020, UChicago Argonne, LLC. All rights reserved.
#  Released under the modified BSD license. See COPYING.md for more details.

import logging
import os
from typing import Dict, Union, List

import pandas as pd

from miplearn.instance import Instance
from miplearn.solvers.learning import LearningSolver
from miplearn.types import LearningSolveStats


class BenchmarkRunner:
    &#34;&#34;&#34;
    Utility class that simplifies the task of comparing the performance of different
    solvers.

    Example
    -------
    ```python
    benchmark = BenchmarkRunner({
        &#34;Baseline&#34;: LearningSolver(...),
        &#34;Strategy A&#34;: LearningSolver(...),
        &#34;Strategy B&#34;: LearningSolver(...),
        &#34;Strategy C&#34;: LearningSolver(...),
    })
    benchmark.fit(train_instances)
    benchmark.parallel_solve(test_instances, n_jobs=5)
    benchmark.save_results(&#34;result.csv&#34;)
    ```

    Parameters
    ----------
    solvers: Dict[str, LearningSolver]
        Dictionary containing the solvers to compare. Solvers may have different
        arguments and components. The key should be the name of the solver. It
        appears in the exported tables of results.
    &#34;&#34;&#34;

    def __init__(self, solvers: Dict[str, LearningSolver]) -&gt; None:
        self.solvers: Dict[str, LearningSolver] = solvers
        self.results = pd.DataFrame(
            columns=[
                &#34;Solver&#34;,
                &#34;Instance&#34;,
            ]
        )

    def parallel_solve(
        self,
        instances: Union[List[str], List[Instance]],
        n_jobs: int = 1,
        n_trials: int = 3,
    ) -&gt; None:
        &#34;&#34;&#34;
        Solves the given instances in parallel and collect benchmark statistics.

        Parameters
        ----------
        instances: Union[List[str], List[Instance]]
            List of instances to solve. This can either be a list of instances
            already loaded in memory, or a list of filenames pointing to pickled (and
            optionally gzipped) files.
        n_jobs: int
            List of instances to solve in parallel at a time.
        n_trials: int
            How many times each instance should be solved.
        &#34;&#34;&#34;
        self._silence_miplearn_logger()
        trials = instances * n_trials
        for (solver_name, solver) in self.solvers.items():
            results = solver.parallel_solve(
                trials,
                n_jobs=n_jobs,
                label=&#34;Solve (%s)&#34; % solver_name,
                discard_outputs=True,
            )
            for i in range(len(trials)):
                idx = i % len(instances)
                results[i][&#34;Solver&#34;] = solver_name
                results[i][&#34;Instance&#34;] = idx
                self.results = self.results.append(pd.DataFrame([results[i]]))
        self._restore_miplearn_logger()

    def write_csv(self, filename: str) -&gt; None:
        &#34;&#34;&#34;
        Writes the collected results to a CSV file.

        Parameters
        ----------
        filename: str
            The name of the file.
        &#34;&#34;&#34;
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        self.results.to_csv(filename)

    def fit(self, instances: Union[List[str], List[Instance]]) -&gt; None:
        &#34;&#34;&#34;
        Trains all solvers with the provided training instances.

        Parameters
        ----------
        instances:  Union[List[str], List[Instance]]
            List of training instances. This can either be a list of instances
            already loaded in memory, or a list of filenames pointing to pickled (and
            optionally gzipped) files.

        &#34;&#34;&#34;
        for (solver_name, solver) in self.solvers.items():
            solver.fit(instances)

    def _silence_miplearn_logger(self) -&gt; None:
        miplearn_logger = logging.getLogger(&#34;miplearn&#34;)
        self.prev_log_level = miplearn_logger.getEffectiveLevel()
        miplearn_logger.setLevel(logging.WARNING)

    def _restore_miplearn_logger(self) -&gt; None:
        miplearn_logger = logging.getLogger(&#34;miplearn&#34;)
        miplearn_logger.setLevel(self.prev_log_level)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="miplearn.benchmark.BenchmarkRunner"><code class="flex name class">
<span>class <span class="ident">BenchmarkRunner</span></span>
<span>(</span><span>solvers)</span>
</code></dt>
<dd>
<section class="desc"><p>Utility class that simplifies the task of comparing the performance of different
solvers.</p>
<h2 id="example">Example</h2>
<pre><code class="language-python">benchmark = BenchmarkRunner({
    &quot;Baseline&quot;: LearningSolver(...),
    &quot;Strategy A&quot;: LearningSolver(...),
    &quot;Strategy B&quot;: LearningSolver(...),
    &quot;Strategy C&quot;: LearningSolver(...),
})
benchmark.fit(train_instances)
benchmark.parallel_solve(test_instances, n_jobs=5)
benchmark.save_results(&quot;result.csv&quot;)
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>solvers</code></strong> :&ensp;<code>Dict</code>[<code>str</code>, <code>LearningSolver</code>]</dt>
<dd>Dictionary containing the solvers to compare. Solvers may have different
arguments and components. The key should be the name of the solver. It
appears in the exported tables of results.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BenchmarkRunner:
    &#34;&#34;&#34;
    Utility class that simplifies the task of comparing the performance of different
    solvers.

    Example
    -------
    ```python
    benchmark = BenchmarkRunner({
        &#34;Baseline&#34;: LearningSolver(...),
        &#34;Strategy A&#34;: LearningSolver(...),
        &#34;Strategy B&#34;: LearningSolver(...),
        &#34;Strategy C&#34;: LearningSolver(...),
    })
    benchmark.fit(train_instances)
    benchmark.parallel_solve(test_instances, n_jobs=5)
    benchmark.save_results(&#34;result.csv&#34;)
    ```

    Parameters
    ----------
    solvers: Dict[str, LearningSolver]
        Dictionary containing the solvers to compare. Solvers may have different
        arguments and components. The key should be the name of the solver. It
        appears in the exported tables of results.
    &#34;&#34;&#34;

    def __init__(self, solvers: Dict[str, LearningSolver]) -&gt; None:
        self.solvers: Dict[str, LearningSolver] = solvers
        self.results = pd.DataFrame(
            columns=[
                &#34;Solver&#34;,
                &#34;Instance&#34;,
            ]
        )

    def parallel_solve(
        self,
        instances: Union[List[str], List[Instance]],
        n_jobs: int = 1,
        n_trials: int = 3,
    ) -&gt; None:
        &#34;&#34;&#34;
        Solves the given instances in parallel and collect benchmark statistics.

        Parameters
        ----------
        instances: Union[List[str], List[Instance]]
            List of instances to solve. This can either be a list of instances
            already loaded in memory, or a list of filenames pointing to pickled (and
            optionally gzipped) files.
        n_jobs: int
            List of instances to solve in parallel at a time.
        n_trials: int
            How many times each instance should be solved.
        &#34;&#34;&#34;
        self._silence_miplearn_logger()
        trials = instances * n_trials
        for (solver_name, solver) in self.solvers.items():
            results = solver.parallel_solve(
                trials,
                n_jobs=n_jobs,
                label=&#34;Solve (%s)&#34; % solver_name,
                discard_outputs=True,
            )
            for i in range(len(trials)):
                idx = i % len(instances)
                results[i][&#34;Solver&#34;] = solver_name
                results[i][&#34;Instance&#34;] = idx
                self.results = self.results.append(pd.DataFrame([results[i]]))
        self._restore_miplearn_logger()

    def write_csv(self, filename: str) -&gt; None:
        &#34;&#34;&#34;
        Writes the collected results to a CSV file.

        Parameters
        ----------
        filename: str
            The name of the file.
        &#34;&#34;&#34;
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        self.results.to_csv(filename)

    def fit(self, instances: Union[List[str], List[Instance]]) -&gt; None:
        &#34;&#34;&#34;
        Trains all solvers with the provided training instances.

        Parameters
        ----------
        instances:  Union[List[str], List[Instance]]
            List of training instances. This can either be a list of instances
            already loaded in memory, or a list of filenames pointing to pickled (and
            optionally gzipped) files.

        &#34;&#34;&#34;
        for (solver_name, solver) in self.solvers.items():
            solver.fit(instances)

    def _silence_miplearn_logger(self) -&gt; None:
        miplearn_logger = logging.getLogger(&#34;miplearn&#34;)
        self.prev_log_level = miplearn_logger.getEffectiveLevel()
        miplearn_logger.setLevel(logging.WARNING)

    def _restore_miplearn_logger(self) -&gt; None:
        miplearn_logger = logging.getLogger(&#34;miplearn&#34;)
        miplearn_logger.setLevel(self.prev_log_level)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="miplearn.benchmark.BenchmarkRunner.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, instances)</span>
</code></dt>
<dd>
<section class="desc"><p>Trains all solvers with the provided training instances.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>instances</code></strong> :&ensp; <code>Union</code>[<code>List</code>[<code>str</code>], <code>List</code>[<code>Instance</code>]]</dt>
<dd>List of training instances. This can either be a list of instances
already loaded in memory, or a list of filenames pointing to pickled (and
optionally gzipped) files.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, instances: Union[List[str], List[Instance]]) -&gt; None:
    &#34;&#34;&#34;
    Trains all solvers with the provided training instances.

    Parameters
    ----------
    instances:  Union[List[str], List[Instance]]
        List of training instances. This can either be a list of instances
        already loaded in memory, or a list of filenames pointing to pickled (and
        optionally gzipped) files.

    &#34;&#34;&#34;
    for (solver_name, solver) in self.solvers.items():
        solver.fit(instances)</code></pre>
</details>
</dd>
<dt id="miplearn.benchmark.BenchmarkRunner.parallel_solve"><code class="name flex">
<span>def <span class="ident">parallel_solve</span></span>(<span>self, instances, n_jobs=1, n_trials=3)</span>
</code></dt>
<dd>
<section class="desc"><p>Solves the given instances in parallel and collect benchmark statistics.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>instances</code></strong> :&ensp;<code>Union</code>[<code>List</code>[<code>str</code>], <code>List</code>[<code>Instance</code>]]</dt>
<dd>List of instances to solve. This can either be a list of instances
already loaded in memory, or a list of filenames pointing to pickled (and
optionally gzipped) files.</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>int</code></dt>
<dd>List of instances to solve in parallel at a time.</dd>
<dt><strong><code>n_trials</code></strong> :&ensp;<code>int</code></dt>
<dd>How many times each instance should be solved.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parallel_solve(
    self,
    instances: Union[List[str], List[Instance]],
    n_jobs: int = 1,
    n_trials: int = 3,
) -&gt; None:
    &#34;&#34;&#34;
    Solves the given instances in parallel and collect benchmark statistics.

    Parameters
    ----------
    instances: Union[List[str], List[Instance]]
        List of instances to solve. This can either be a list of instances
        already loaded in memory, or a list of filenames pointing to pickled (and
        optionally gzipped) files.
    n_jobs: int
        List of instances to solve in parallel at a time.
    n_trials: int
        How many times each instance should be solved.
    &#34;&#34;&#34;
    self._silence_miplearn_logger()
    trials = instances * n_trials
    for (solver_name, solver) in self.solvers.items():
        results = solver.parallel_solve(
            trials,
            n_jobs=n_jobs,
            label=&#34;Solve (%s)&#34; % solver_name,
            discard_outputs=True,
        )
        for i in range(len(trials)):
            idx = i % len(instances)
            results[i][&#34;Solver&#34;] = solver_name
            results[i][&#34;Instance&#34;] = idx
            self.results = self.results.append(pd.DataFrame([results[i]]))
    self._restore_miplearn_logger()</code></pre>
</details>
</dd>
<dt id="miplearn.benchmark.BenchmarkRunner.write_csv"><code class="name flex">
<span>def <span class="ident">write_csv</span></span>(<span>self, filename)</span>
</code></dt>
<dd>
<section class="desc"><p>Writes the collected results to a CSV file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>The name of the file.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_csv(self, filename: str) -&gt; None:
    &#34;&#34;&#34;
    Writes the collected results to a CSV file.

    Parameters
    ----------
    filename: str
        The name of the file.
    &#34;&#34;&#34;
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    self.results.to_csv(filename)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="miplearn" href="index.html">miplearn</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="miplearn.benchmark.BenchmarkRunner" href="#miplearn.benchmark.BenchmarkRunner">BenchmarkRunner</a></code></h4>
<ul class="">
<li><code><a title="miplearn.benchmark.BenchmarkRunner.fit" href="#miplearn.benchmark.BenchmarkRunner.fit">fit</a></code></li>
<li><code><a title="miplearn.benchmark.BenchmarkRunner.parallel_solve" href="#miplearn.benchmark.BenchmarkRunner.parallel_solve">parallel_solve</a></code></li>
<li><code><a title="miplearn.benchmark.BenchmarkRunner.write_csv" href="#miplearn.benchmark.BenchmarkRunner.write_csv">write_csv</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.5</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>